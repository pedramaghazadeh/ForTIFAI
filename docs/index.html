<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header class="header">
            <h1>ForTIFAI</h1>
            <h2>Fending Off Recursive Training Induced Failure for AI Models</h2>
        </header>

        <main class="main-content">
            <!-- Authors -->
            <div class="authors">
                <p><strong>Soheil Zibakhsh Shabgahi</strong><sup>1</sup>, <strong>Pedram Aghazadeh</strong><sup>1</sup>, <strong>Azalia Mirhoseini</strong><sup>2</sup>, <strong>Farinaz Koushanfar</strong><sup>1</sup></p>
                <p><sup>1</sup>University of California, San Diego &nbsp;&nbsp; <sup>2</sup>Stanford University</p>
            </div>

            <!-- Links -->
            <div class="links">
                <a href="https://arxiv.org/abs/2509.08972" class="link">[Paper]</a>
                <a href="https://github.com/pedramaghazadeh/fortifai" class="link">[Code]</a>
                <a href="#abstract" class="link">[Abstract]</a>
            </div>

            <!-- Main Figure -->
            <div class="figure-container">
                <img src="https://www.pedramag.xyz/assets/publications/fortifai.jpg" alt="ForTIFAI Method Overview" class="main-figure">
                <p class="figure-caption">
                    <strong>Our proposed framework for monitoring model performance across generations in a realistic mixed-data setting while trained on previous generations' data.</strong></p>
            </div>

            <!-- Abstract -->
            <section id="abstract" class="section">
                <h3>Abstract</h3>
                <p>
                    The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, 
                    with some projections suggesting that most available new data for training could be machine-generated by 2030. 
                    This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data 
                    leads to a phenomenon known as <strong>model collapse</strong>, where model performance degrades over generations 
                    of training, eventually rendering the models ineffective.
                </p>
                <p>
                    We address this challenge by leveraging a key insight: auto-regressive models tend to generate text sequences 
                    to which they assign high confidence (i.e., high log-likelihood). Based on this observation, we introduce 
                    the <strong>Truncated-Cross-Entropy (TCE) loss function</strong>. TCE mitigates collapse by selectively ignoring 
                    high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from 
                    the learning process.
                </p>
                <p>
                    Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit 
                    significantly increased resilience, tolerating over <strong>2.3x more synthetic data</strong> before the onset of collapse. 
                    In addition, we provide an open-source benchmark for collapse dynamics in mixed-data settings.
                </p>
            </section>

            <!-- Method -->
            <section class="section">
                <h3>Method</h3>
                <p>
                    We use the Truncated-Cross-Entropy (TCE) loss function to filter high-confidence tokens during training. 
                    The TCE loss is defined as:
                </p>
                <div class="formula">
                    L<sub>TCE</sub> = -∑<sub>i=1</sub><sup>N</sup> log p(x<sub>i</sub>) · I[log p(x<sub>i</sub>) < τ]
                </div>
                <p>
                    where τ is the confidence threshold and I[·] is the indicator function. This approach prevents the model 
                    from learning to reproduce high-confidence synthetic artifacts while preserving useful information from 
                    low-confidence tokens.
                </p>
            </section>

            <!-- Citation -->
            <section class="section">
                <h3>Citation</h3>
                <div class="citation">
                    <pre>@article{shabgahi2025fortifai,
  title={ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models},
  author={Shabgahi, Soheil Zibakhsh and Aghazadeh, Pedram and Mirhoseini, Azalia and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:2509.08972},
  year={2025}
}</pre>
                </div>
            </section>
        </main>

        <footer class="footer">
            <p>Correspondence to: <a href="mailto:paghazadeh@ucsd.edu">paghazadeh@ucsd.edu</a></p>
        </footer>
    </div>
</body>
</html>
