# ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models

[![Paper](https://img.shields.io/badge/Paper-arXiv%3A2509.08972-red)](https://arxiv.org/abs/2509.08972)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

This repository contains the implementation for **ForTIFAI**, a novel approach to mitigate model collapse in AI systems trained on synthetic data. The work addresses the critical challenge of recursive training-induced failure (TIF) where models degrade over generations when trained on machine-generated content.

## ğŸ“– Abstract

The increasing reliance on generative AI models is rapidly increasing the volume of synthetic data, with some projections suggesting that most available new data for training could be machine-generated by 2030. This shift to a mainly synthetic content presents a critical challenge: repeated training in synthetic data leads to a phenomenon known as model collapse, where model performance degrades over generations of training, eventually rendering the models ineffective.

**ForTIFAI** introduces the **Truncated-Cross-Entropy (TCE) loss function** that mitigates collapse by selectively ignoring high-confidence tokens during training, effectively filtering out likely machine-generated artifacts from the learning process. Our experiments demonstrate that models trained with TCE not only learn effectively but also exhibit significantly increased resilience, tolerating over 2.3x more synthetic data before the onset of collapse.

## ğŸš€ Key Features

- **Truncated-Cross-Entropy (TCE) Loss**: Novel loss function that filters high-confidence tokens to prevent model collapse
- **Multi-Stage Training Pipeline**: Iterative training process that simulates recursive training scenarios
- **Comprehensive Evaluation**: BLIMP, GSM8K, HellaSwag, and custom knowledge tracing benchmarks
- **AWS Integration**: Scalable cloud-based training and evaluation
- **Multiple Model Support**: Compatible with LLaMA, Gemma, DeepSeek, and other transformer models

## ğŸ“ Repository Structure

```
ForTIFAI/
â”œâ”€â”€ src/                          # Main source code
â”‚   â”œâ”€â”€ pipeline.py              # Main training pipeline orchestrator
â”‚   â”œâ”€â”€ train.py                 # Training implementation
â”‚   â”œâ”€â”€ generate.py              # Text generation utilities
â”‚   â”œâ”€â”€ losses.py                # TCE and other loss functions
â”‚   â”œâ”€â”€ dataset.py               # Dataset handling and preprocessing
â”‚   â”œâ”€â”€ evaluate.py              # Model evaluation utilities
â”‚   â”œâ”€â”€ convert_model.py         # Model format conversion
â”‚   â”œâ”€â”€ aws.py                   # AWS S3 integration
â”‚   â”œâ”€â”€ run_pipeline.sh          # Local training script
â”‚   â”œâ”€â”€ aws_run_pipeline.sh      # AWS training script
â”‚   â”œâ”€â”€ eval.sh                  # Evaluation script
â”‚   â””â”€â”€ data/                    # Dataset storage
â”‚       â”œâ”€â”€ wikitext2/          # WikiText-2 dataset
â”‚       â”œâ”€â”€ MNIST/              # MNIST dataset
â”‚       â””â”€â”€ blimpadjunct_island_causative/  # BLIMP evaluation data
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ› ï¸ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/your-username/ForTIFAI.git
cd ForTIFAI
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables (optional for AWS):**
```bash
export AWS_ACCESS_KEY_ID="your_access_key"
export AWS_SECRET_ACCESS_KEY="your_secret_key"
export WANDB_API_KEY="your_wandb_key"
```

## ğŸƒâ€â™‚ï¸ Quick Start

### Local Training

Run the complete training pipeline locally:

```bash
cd src
./run_pipeline.sh
```

### AWS Training

For distributed training on AWS:

```bash
cd src
./aws_run_pipeline.sh --aws-access-key YOUR_KEY --aws-secret-key YOUR_SECRET
```

### Evaluation

Evaluate trained models:

```bash
cd src
./eval.sh --path /path/to/models --name model_suffix
```

## ğŸ”§ Pipeline Overview

The ForTIFAI pipeline consists of multiple stages that simulate recursive training:

### Stage 0: Initial Training
- Train model on original human data
- Establish baseline performance
- Generate initial synthetic data

### Stages 1-N: Recursive Training
- Train on mixed human + synthetic data
- Apply TCE loss to filter high-confidence tokens
- Generate new synthetic data for next stage
- Evaluate model performance degradation

### Key Components

1. **Truncated-Cross-Entropy Loss**: The core innovation that prevents model collapse
2. **Multi-Stage Pipeline**: Simulates real-world recursive training scenarios
3. **Comprehensive Evaluation**: Multiple benchmarks to assess model degradation

## ğŸ“Š Supported Models

- **LLaMA 3.2 1B** (`meta-llama/Llama-3.2-1B`)
- **Gemma 3 1B** (`google/gemma-3-1b-pt`)
- **DeepSeek R1 Distill** (`deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`)
- **Custom models** via Hugging Face integration

## ğŸ¯ Loss Functions

The repository implements several loss functions for comparison:

- **Baseline**: Standard cross-entropy loss
- **TCE (Truncated-Cross-Entropy)**: Our proposed method
- **Focal Loss**: Alternative approach for handling imbalanced data
- **Clipped Loss**: Modified version with confidence clipping

## ğŸ“ˆ Evaluation Benchmarks

- **BLIMP**: Linguistic acceptability judgments
- **GSM8K**: Mathematical reasoning
- **HellaSwag**: Commonsense reasoning
- **WikiText-2**: Language modeling perplexity
- **Knowledge Tracing**: Custom evaluation for synthetic data quality

## ğŸ”¬ Experimental Results

Our experiments demonstrate that:

- **TCE-trained models** tolerate **2.3x more synthetic data** before collapse
- **Significant delay** in performance degradation across multiple benchmarks
- **Robust performance** across different model architectures and sizes

## ğŸ“ Usage Examples

### Basic Training
```bash
# Train with TCE loss
./run_pipeline.sh --loss-type clipped_0.99 --model-tag meta-llama/Llama-3.2-1B

# Train with focal loss
./run_pipeline.sh --loss-type focal_2.0 --focal-gamma 2.0
```

### Custom Configuration
```bash
# Custom training parameters
./run_pipeline.sh \
  --model-tag google/gemma-3-1b-pt \
  --batch-size 32 \
  --learning-rate 1e-5 \
  --max-epochs 15 \
  --dataset-fractions 8
```

### Evaluation
```bash
# Evaluate specific model stages
./eval.sh --path /path/to/checkpoints --name clipped_0.99 --device 0
```

## ğŸ§ª Advanced Features

### Custom Loss Functions
Implement your own loss functions by extending the `losses.py` module:

```python
def custom_loss(logits, labels, gamma=0.99):
    # Your custom loss implementation
    pass
```

### Dataset Integration
Add new datasets by extending the `dataset.py` module:

```python
class CustomDataset(Text2Dataset):
    def __init__(self, data_path, tokenizer):
        # Your dataset implementation
        pass
```

## ğŸ“Š Monitoring and Logging

The pipeline includes comprehensive logging:

- **Weights & Biases**: Training metrics and model tracking
- **TensorBoard**: Local training visualization
- **AWS S3**: Model checkpoint storage and retrieval
- **Custom Metrics**: Collapse detection and performance monitoring

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ Citation

If you use this code in your research, please cite our paper:

```bibtex
@article{shabgahi2025fortifai,
  title={ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models},
  author={Shabgahi, Soheil Zibakhsh and Aghazadeh, Pedram and Mirhoseini, Azalia and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:2509.08972},
  year={2025}
}
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- The authors thank the open-source community for the foundational models and datasets
- Special thanks to the Hugging Face team for the transformers library
- AWS credits provided for large-scale experiments

## ğŸ“ Contact

- **Soheil Zibakhsh Shabgahi**: [GitHub](https://github.com/soheilzibakhsh)
- **Pedram Aghazadeh**: [GitHub](https://github.com/pedramaghazadeh)
- **Paper**: [arXiv:2509.08972](https://arxiv.org/abs/2509.08972)

## ğŸ”— Related Work

- [Model Collapse Paper](https://arxiv.org/abs/2305.17493)
- [Synthetic Data Generation](https://arxiv.org/abs/2301.07514)
- [Recursive Training Analysis](https://arxiv.org/abs/2309.00071)

---

**âš ï¸ Important Note**: This implementation is for research purposes. Please ensure you have appropriate computational resources and follow responsible AI practices when conducting experiments.
